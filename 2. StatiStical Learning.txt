Y = f(X) + e

Here f is some fixed but unknown function of X1, . . . ,Xp, 
and e is a random error term, which is independent of X and has mean zero. 

In this formulation, f represents the systematic information that X provides about Y.

In essence, statistical learning refers to a set of approaches for estimating f.

reasons to estimate: prediction and inference

Prediction -

Y^ = f^(X)

f^ -> estimate of f -> black box
Y^ -> resulting prediction for Y


Accuracy of Y^ as a prediction of Y depends on two quantities 

- Reducible error
	- improve accuracy of f^
	- Y^ = f(X)

- Irreducible error
	- variability associated with e
	- by definition independent of X
	- may contain unmeasurable variation



Inference -

 - understand association between Y and X1,X2,..,Xp.
 - f^ is not a black box

examples of inference problems -
 – Which media are associated with sales?
 – Which media generate the biggest boost in sales? or
 – How large of an increase in sales is associated with a given increase in TV advertising?


Depending on whether our ultimate goal is prediction, inference, or a combination of the two, different methods for estimating f may be appropriate.


Most statistical learning methods for estimating f can be characterised as either parametric or non-parametric.

Parametric Methods

 - 2 step model-based approach

 1. we make assumption about the function form or shape of f.
	for example - simple assumption that f is linear in X
		f(X) = B0 + B1X1 +B2X2 + ...+ BpXp

	This is a linear model. once assumed, estimating f is simplified.
	Instead of estimating p-dimensional function f(X), one only needs to estimate the p+1 coefficients B0,B1,...,Bp

 2. after a model is selected, we need procedure that uses training data to fit or train model.
	for linear model, need to estimate parameters  B0,B1,...,Bp
	The most common approach to fitting the model is referred to as (ordinary) least squares. one of many possible ways to fit the linear model

parametric - reduce problem of estimating f down to one of estimating a set of parameters.

advantage - simplifies the problem of estimating f -> estimate parameters than arbitrary function f

disadvantage - choosen model will usually not match true unknown form of f
	- can try to address the problem using flexible models
		- may require estimating greater number of parameters
	- can lead to overfitting of the data


Non-Parametric Methods

 - do not make explicit assumptions about the function form of f
 - seek to estimate f without being too rough or wiggly

advantage - potential to accurately fit a wider range of possible shapes for f

disadvantage - since do not reduce the problem to estimating small number of parameters, a very large number of observations is rquired to obtain an accurate estimate of f


In general, as the flexibility of a method increases, its interpretability decreases.

we might choose restrictive method instead of flexible approach for
	- inference - more interpretable

interpretability (high->low), flexibility (low->high)

- Lasso - relies upon linear model, sets number of coefficients to zero, less flexible tham LM but more interpretables
- Least Squares
- Generalized additive models (GAMs) - extends LM for certain non-linear relationships, more flexible but less interpretable tham LM
- bagging, boosting - non-linear methods
- SVM
- Neural Networks (Deep Learning) - highly flexible, harder to interpret



Most Statistical learning problems fall into one of the two categories - supervised or unsupervised


Supervised Learning methods
	- aim to accurately predict response for future observations (prediction)
	- better understand the relationship between the response and the predictors (inference)

	- classical statistical learning methods = linear and logistic regression
	- modern approaches - GAMs, bagging, boosting SVM



Unsupervised Learning Methods
	- can seek to understand relationships between the observations

	- clustering or cluster analysis


Semi-supervised learning problem
In this setting, we wish to use a sta- semisupervised learning method that can incorporate the m observations for which response measurements are available as well as the n −m observations for which they are not.



Variables - quantitative or qualitative (categorical)

problems with quatitative response - regression problem - least square
	 with qualitative response - classification problem - logistic regression (two-class or binary)

Logistic regression predicts class probabilities - can be thought of as a regression method as well

K-nearest neighbors and boosting can be used for either quantitative or qualitative responses.



Measuring the Quality of fit

- In regression setting, the most commonly used measure is mean squared error (MSE) = 1/n E(yi - f^(xi))^2
- we want models that predict lower "test" MSE and not the ones that have lower "train" MSE
	- no guarantee that the method with the lowest train MSE will also have lowest test MSE
	- problem is that many statistical methods generate coefficient to minimize the train MSE
 
The degrees of freedom is a quantity that summarizes the flexibility of a curve.

As model flexibility increases, the training MSE will decrease, but the test MSE may not.  When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data.

Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.

Cross-Validation - method for estimating test MSE using training data.


Bias-Variance Trade off

expected test MSE, for a given value x0, can always be decomposed into the sum of three fundamental quantities: the variance of f^(x0), the squared bias of  f^(x0) and the variance of the error variance terms e.

	E(y0 − f^(x0)^2 = Var( f^(x0)) + [Bias(  f^(x0))]^2 + Var(e).

Variance refers to the amount by which f^ would change if we estimated it using a different training data set.
In general, more flexible statistical methods have higher variance.

bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.
Generally, more flexible methods result in less bias.

As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease.

The relative rate of change of these two quantities determines whether the test MSE increases or decreases.

 As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. 
However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. 


